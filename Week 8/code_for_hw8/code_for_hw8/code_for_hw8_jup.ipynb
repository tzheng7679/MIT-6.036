{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code file pertains to problems 3 onwards from homework 8\n",
    "# Here, we use out-of-the-box neural network frameworks Keras and Tensorflow \n",
    "# to build and train our models\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import math as m \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "from keras.initializers import VarianceScaling\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Problem 3 - 2D data\n",
    "######################################################################\n",
    "\n",
    "def archs(classes):\n",
    "    return [[Dense(input_dim=2, units=classes, activation=\"softmax\")],\n",
    "            [Dense(input_dim=2, units=10, activation='relu'),\n",
    "             Dense(units=classes, activation=\"softmax\")],\n",
    "            [Dense(input_dim=2, units=100, activation='relu'),\n",
    "             Dense(units=classes, activation=\"softmax\")],\n",
    "            [Dense(input_dim=2, units=10, activation='relu'),\n",
    "             Dense(units=10, activation='relu'),\n",
    "             Dense(units=classes, activation=\"softmax\")],\n",
    "            [Dense(input_dim=2, units=100, activation='relu'),\n",
    "             Dense(units=100, activation='relu'),\n",
    "             Dense(units=classes, activation=\"softmax\")]]\n",
    "\n",
    "# Read the simple 2D dataset files\n",
    "def get_data_set(name):\n",
    "    try:\n",
    "        data = np.loadtxt(name, skiprows=0, delimiter = ' ')\n",
    "    except:\n",
    "        return None, None, None\n",
    "    np.random.shuffle(data)             # shuffle the data\n",
    "    # The data uses ROW vectors for a data point, that's what Keras assumes.\n",
    "    _, d = data.shape\n",
    "    X = data[:,0:d-1]\n",
    "    Y = data[:,d-1:d]\n",
    "    y = Y.T[0]\n",
    "    classes = set(y)\n",
    "    if classes == set([-1.0, 1.0]):\n",
    "        print('Convert from -1,1 to 0,1')\n",
    "        y = 0.5*(y+1)\n",
    "    print('Loading X', X.shape, 'y', y.shape, 'classes', set(y))\n",
    "    return X, y, len(classes)\n",
    "\n",
    "######################################################################\n",
    "# General helpers for Problems 3-5\n",
    "######################################################################\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.keys = ['loss', 'acc', 'val_loss', 'val_acc']\n",
    "        self.values = {}\n",
    "        for k in self.keys:\n",
    "            self.values['batch_'+k] = []\n",
    "            self.values['epoch_'+k] = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        for k in self.keys:\n",
    "            bk = 'batch_'+k\n",
    "            if k in logs:\n",
    "                self.values[bk].append(logs[k])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        for k in self.keys:\n",
    "            ek = 'epoch_'+k\n",
    "            if k in logs:\n",
    "                self.values[ek].append(logs[k])\n",
    "\n",
    "    def plot(self, keys):\n",
    "        for key in keys:\n",
    "            plt.plot(np.arange(len(self.values[key])), np.array(self.values[key]), label=key)\n",
    "        plt.legend()\n",
    "\n",
    "def run_keras(X_train, y_train, X_val, y_val, X_test, y_test, layers, epochs, split=0, verbose=True):\n",
    "    # Model specification\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    # Define the optimization\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "    N = X_train.shape[0]\n",
    "    # Pick batch size\n",
    "    batch = 32 if N > 1000 else 1     # batch size\n",
    "    history = LossHistory()\n",
    "    # Fit the model\n",
    "    if X_val is None:\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch, validation_split=split,\n",
    "                  callbacks=[history], verbose=verbose)\n",
    "    else:\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch, validation_data=(X_val, y_val),\n",
    "                  callbacks=[history], verbose=verbose)\n",
    "    # Evaluate the model on validation data, if any\n",
    "    if X_val is not None or split > 0:\n",
    "        val_acc, val_loss = history.values['epoch_val_acc'][-1], history.values['epoch_val_loss'][-1]\n",
    "        print (\"\\nLoss on validation set:\"  + str(val_loss) + \" Accuracy on validation set: \" + str(val_acc))\n",
    "    else:\n",
    "        val_acc = None\n",
    "    # Evaluate the model on test data, if any\n",
    "    if X_test is not None:\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=batch)\n",
    "        print (\"\\nLoss on test set:\"  + str(test_loss) + \" Accuracy on test set: \" + str(test_acc))\n",
    "    else:\n",
    "        test_acc = None\n",
    "    return model, history, val_acc, test_acc\n",
    "\n",
    "def dataset_paths(data_name):\n",
    "    return [\"data/data\"+data_name+\"_\"+suffix+\".csv\" for suffix in (\"train\", \"validate\", \"test\")]\n",
    "\n",
    "# The name is a string such as \"1\" or \"Xor\"\n",
    "def run_keras_2d(data_name, layers, epochs, display=True, split=0.25, verbose=True, trials=1):\n",
    "    print('Keras FC: dataset=', data_name)\n",
    "    (train_dataset, val_dataset, test_dataset) = dataset_paths(data_name)\n",
    "    # Load the datasets\n",
    "    X_train, y, num_classes = get_data_set(train_dataset)\n",
    "    X_val, y2, _ = get_data_set(val_dataset)\n",
    "    X_test, y3, _ = get_data_set(test_dataset)\n",
    "    # Categorize the labels\n",
    "    y_train = np_utils.to_categorical(y, num_classes) # one-hot\n",
    "    y_val = y_test = None\n",
    "    if X_val is not None:\n",
    "        y_val = np_utils.to_categorical(y2, num_classes) # one-hot        \n",
    "    if X_test is not None:\n",
    "        y_test = np_utils.to_categorical(y3, num_classes) # one-hot\n",
    "    val_acc, test_acc = 0, 0\n",
    "    for trial in range(trials):\n",
    "        # Reset the weights\n",
    "        # See https://github.com/keras-team/keras/issues/341\n",
    "        session = K.get_session()\n",
    "        for layer in layers:\n",
    "            for v in layer.__dict__:\n",
    "                v_arg = getattr(layer, v)\n",
    "                if hasattr(v_arg, 'initializer'):\n",
    "                    initializer_func = getattr(v_arg, 'initializer')\n",
    "                    initializer_func.run(session=session)\n",
    "        # Run the model\n",
    "        model, history, vacc, tacc, = \\\n",
    "               run_keras(X_train, y_train, X_val, y_val, X_test, y_test, layers, epochs,\n",
    "                         split=split, verbose=verbose)\n",
    "        val_acc += vacc if vacc else 0\n",
    "        test_acc += tacc if tacc else 0\n",
    "        if display:\n",
    "            # plot classifier landscape on training data\n",
    "            plot_heat(X_train, y, model)\n",
    "            plt.title('Training data')\n",
    "            plt.show()\n",
    "            if X_test is not None:\n",
    "                # plot classifier landscape on testing data\n",
    "                plot_heat(X_test, y3, model)\n",
    "                plt.title('Testing data')\n",
    "                plt.show()\n",
    "            # Plot epoch loss\n",
    "            history.plot(['epoch_loss', 'epoch_val_loss'])\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.title('Epoch val_loss and loss')\n",
    "            plt.show()\n",
    "            # Plot epoch accuracy\n",
    "            history.plot(['epoch_acc', 'epoch_val_acc'])\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.title('Epoch val_acc and acc')\n",
    "            plt.show()\n",
    "    if val_acc:\n",
    "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
    "    if test_acc:\n",
    "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
    "    return X_train, y, model\n",
    "\n",
    "######################################################################\n",
    "# Helper functions for \n",
    "# OPTIONAL: Problem 4 - Weight Sharing\n",
    "######################################################################\n",
    "\n",
    "def generate_1d_images(nsamples,image_size,prob):\n",
    "    Xs=[]\n",
    "    Ys=[]\n",
    "    for i in range(0,nsamples):\n",
    "        X=np.random.binomial(1, prob, size=image_size)\n",
    "        Y=count_objects_1d(X)\n",
    "        Xs.append(X)\n",
    "        Ys.append(Y)\n",
    "    Xs=np.array(Xs)\n",
    "    Ys=np.array(Ys)\n",
    "    return Xs,Ys\n",
    "\n",
    "\n",
    "#count the number of objects in a 1d array\n",
    "def count_objects_1d(array):\n",
    "    count=0\n",
    "    for i in range(len(array)):\n",
    "        num=array[i]\n",
    "        if num==0:\n",
    "            if i==0 or array[i-1]==1:\n",
    "                count+=1\n",
    "    return count\n",
    "\n",
    "def l1_reg(weight_matrix):\n",
    "    return 0.01 * K.sum(K.abs(weight_matrix))    \n",
    "\n",
    "\n",
    "def filter_reg(weights):\n",
    "    lam=0\n",
    "    return lam* val\n",
    "\n",
    "def get_image_data_1d(tsize,image_size,prob):\n",
    "    #prob controls the density of white pixels\n",
    "    #tsize is the size of the training and test sets\n",
    "    vsize=int(0.2*tsize)\n",
    "    X_train,Y_train=generate_1d_images(tsize,image_size,prob)\n",
    "    X_val,Y_val=generate_1d_images(vsize,image_size,prob)\n",
    "    X_test,Y_test=generate_1d_images(tsize,image_size,prob)\n",
    "    #reshape the input data for the convolutional layer\n",
    "    X_train=np.expand_dims(X_train,axis=2)\n",
    "    X_val=np.expand_dims(X_val,axis=2)\n",
    "    X_test=np.expand_dims(X_test,axis=2)\n",
    "    data=(X_train,Y_train,X_val,Y_val,X_test,Y_test)\n",
    "    return data\n",
    "\n",
    "def train_neural_counter(layers,data,loss_func='mse',display=False):\n",
    "    (X_train,Y_train,X_val,Y_val,X_test,Y_test)=data\n",
    "    epochs=10\n",
    "    batch=1\n",
    "    \n",
    "    model=Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    model.summary()    \n",
    "    model.compile(loss=loss_func, optimizer=Adam())\n",
    "    history = LossHistory()    \n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch, validation_data=(X_val, Y_val),callbacks=[history], verbose=True)\n",
    "    err=model.evaluate(X_test,Y_test)\n",
    "    ws=model.layers[-1].get_weights()[0]\n",
    "    if display:\n",
    "        plt.plot(ws)\n",
    "        plt.show()\n",
    "    return model,err\n",
    "\n",
    "######################################################################\n",
    "# Problem 5\n",
    "######################################################################\n",
    "\n",
    "def shifted(X, shift):\n",
    "    n = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    size = m + shift\n",
    "    X_sh = np.zeros((n, size, size))\n",
    "    plt.ion()\n",
    "    for i in range(n):\n",
    "        sh1 = np.random.randint(shift)\n",
    "        sh2 = np.random.randint(shift)\n",
    "        X_sh[i, sh1:sh1+m, sh2:sh2+m] = X[i, :, :]\n",
    "        # If you want to see the shifts, uncomment\n",
    "        #plt.figure(1); plt.imshow(X[i])\n",
    "        #plt.figure(2); plt.imshow(X_sh[i])\n",
    "        #plt.show()\n",
    "        #input('Go?')\n",
    "    return X_sh\n",
    "  \n",
    "def get_MNIST_data(shift=0):\n",
    "    (X_train, y1), (X_val, y2) = mnist.load_data()\n",
    "    if shift:\n",
    "        size = 28+shift\n",
    "        X_train = shifted(X_train, shift)\n",
    "        X_val = shifted(X_val, shift)\n",
    "    return (X_train, y1), (X_val, y2)\n",
    "\n",
    "# Example Usage:\n",
    "# train, validation = get_MNIST_data()\n",
    "\n",
    "def run_keras_fc_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
    "    (X_train, y1), (X_val, y2) = train, test\n",
    "    # Flatten the images\n",
    "    m = X_train.shape[1]\n",
    "    X_train = X_train.reshape((X_train.shape[0], m*m))\n",
    "    X_val = X_val.reshape((X_val.shape[0], m*m))\n",
    "    # Categorize the labels\n",
    "    num_classes = 10\n",
    "    y_train = np_utils.to_categorical(y1, num_classes)\n",
    "    y_val = np_utils.to_categorical(y2, num_classes)\n",
    "    # Train, use split for validation\n",
    "    val_acc, test_acc = 0, 0\n",
    "    for trial in range(trials):\n",
    "        # Reset the weights\n",
    "        # See https://github.com/keras-team/keras/issues/341\n",
    "        session = K.get_session()\n",
    "        for layer in layers:\n",
    "            for v in layer.__dict__:\n",
    "                v_arg = getattr(layer, v)\n",
    "                if hasattr(v_arg, 'initializer'):\n",
    "                    initializer_func = getattr(v_arg, 'initializer')\n",
    "                    initializer_func.run(session=session)\n",
    "        # Run the model\n",
    "        model, history, vacc, tacc = \\\n",
    "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
    "        val_acc += vacc if vacc else 0\n",
    "        test_acc += tacc if tacc else 0\n",
    "    if val_acc:\n",
    "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
    "    if test_acc:\n",
    "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
    "\n",
    "def run_keras_cnn_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
    "    # Load the dataset\n",
    "    (X_train, y1), (X_val, y2) = train, test\n",
    "    # Add a final dimension indicating the number of channels (only 1 here)\n",
    "    m = X_train.shape[1]\n",
    "    X_train = X_train.reshape((X_train.shape[0], m, m, 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], m, m, 1))\n",
    "    # Categorize the labels\n",
    "    num_classes = 10\n",
    "    y_train = np_utils.to_categorical(y1, num_classes)\n",
    "    y_val = np_utils.to_categorical(y2, num_classes)\n",
    "    # Train, use split for validation\n",
    "    val_acc, test_acc = 0, 0\n",
    "    for trial in range(trials):\n",
    "        # Reset the weights\n",
    "        # See https://github.com/keras-team/keras/issues/341\n",
    "        session = K.get_session()\n",
    "        for layer in layers:\n",
    "            for v in layer.__dict__:\n",
    "                v_arg = getattr(layer, v)\n",
    "                if hasattr(v_arg, 'initializer'):\n",
    "                    initializer_func = getattr(v_arg, 'initializer')\n",
    "                    initializer_func.run(session=session)\n",
    "        # Run the model\n",
    "        model, history, vacc, tacc = \\\n",
    "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
    "        val_acc += vacc if vacc else 0\n",
    "        test_acc += tacc if tacc else 0\n",
    "    if val_acc:\n",
    "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
    "    if test_acc:\n",
    "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
    "\n",
    "# Example usage:\n",
    "# train, validation = get_MNIST_data()\n",
    "# layers = [Dense(input_dim=???, units=???, activation='softmax')]\n",
    "# run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)\n",
    "# Same pattern applies to the function: run_keras_cnn_mnist\n",
    "\n",
    "######################################################################\n",
    "# Plotting Functions\n",
    "######################################################################\n",
    "\n",
    "def plot_heat(X, y, model, res = 200):\n",
    "    eps = .1\n",
    "    xmin = np.min(X[:,0]) - eps; xmax = np.max(X[:,0]) + eps\n",
    "    ymin = np.min(X[:,1]) - eps; ymax = np.max(X[:,1]) + eps\n",
    "    ax = tidyPlot(xmin, xmax, ymin, ymax, xlabel = 'x', ylabel = 'y')\n",
    "    xl = np.linspace(xmin, xmax, res)\n",
    "    yl = np.linspace(ymin, ymax, res)\n",
    "    xx, yy = np.meshgrid(xl, yl, sparse=False)\n",
    "    zz = np.argmax(model.predict(np.c_[xx.ravel(), yy.ravel()]), axis=1)\n",
    "    im = ax.imshow(np.flipud(zz.reshape((res,res))), interpolation = 'none',\n",
    "                   extent = [xmin, xmax, ymin, ymax],\n",
    "                   cmap = 'viridis')\n",
    "    plt.colorbar(im)\n",
    "    for yi in set([int(_y) for _y in set(y)]):\n",
    "        color = ['r', 'g', 'b'][yi]\n",
    "        marker = ['X', 'o', 'v'][yi]\n",
    "        cl = np.where(y==yi)\n",
    "        ax.scatter(X[cl,0], X[cl,1], c = color, marker = marker, s=80,\n",
    "                   edgecolors = 'none')\n",
    "    return ax\n",
    "\n",
    "def tidyPlot(xmin, xmax, ymin, ymax, center = False, title = None,\n",
    "                 xlabel = None, ylabel = None):\n",
    "    plt.figure(facecolor=\"white\")\n",
    "    ax = plt.subplot()\n",
    "    if center:\n",
    "        ax.spines['left'].set_position('zero')\n",
    "        ax.spines['right'].set_color('none')\n",
    "        ax.spines['bottom'].set_position('zero')\n",
    "        ax.spines['top'].set_color('none')\n",
    "        ax.spines['left'].set_smart_bounds(True)\n",
    "        ax.spines['bottom'].set_smart_bounds(True)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "    else:\n",
    "        ax.spines[\"top\"].set_visible(False)    \n",
    "        ax.spines[\"right\"].set_visible(False)    \n",
    "        ax.get_xaxis().tick_bottom()  \n",
    "        ax.get_yaxis().tick_left()\n",
    "    eps = .05\n",
    "    plt.xlim(xmin-eps, xmax+eps)\n",
    "    plt.ylim(ymin-eps, ymax+eps)\n",
    "    if title: ax.set_title(title)\n",
    "    if xlabel: ax.set_xlabel(xlabel)\n",
    "    if ylabel: ax.set_ylabel(ylabel)\n",
    "    return ax\n",
    "\n",
    "def plot_separator(ax, th, th_0):\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin,ymax = ax.get_ylim()\n",
    "    pts = []\n",
    "    eps = 1.0e-6\n",
    "    # xmin boundary crossing is when xmin th[0] + y th[1] + th_0 = 0\n",
    "    # that is, y = (-th_0 - xmin th[0]) / th[1]\n",
    "    if abs(th[1,0]) > eps:\n",
    "        pts += [np.array([x, (-th_0 - x * th[0,0]) / th[1,0]]) \\\n",
    "                                                        for x in (xmin, xmax)]\n",
    "    if abs(th[0,0]) > 1.0e-6:\n",
    "        pts += [np.array([(-th_0 - y * th[1,0]) / th[0,0], y]) \\\n",
    "                                                         for y in (ymin, ymax)]\n",
    "    in_pts = []\n",
    "    for p in pts:\n",
    "        if (xmin-eps) <= p[0] <= (xmax+eps) and \\\n",
    "           (ymin-eps) <= p[1] <= (ymax+eps):\n",
    "            duplicate = False\n",
    "            for p1 in in_pts:\n",
    "                if np.max(np.abs(p - p1)) < 1.0e-6:\n",
    "                    duplicate = True\n",
    "            if not duplicate:\n",
    "                in_pts.append(p)\n",
    "    if in_pts and len(in_pts) >= 2:\n",
    "        # Plot separator\n",
    "        vpts = np.vstack(in_pts)\n",
    "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
    "        # Plot normal\n",
    "        vmid = 0.5*(in_pts[0] + in_pts[1])\n",
    "        scale = np.sum(th*th)**0.5\n",
    "        diff = in_pts[0] - in_pts[1]\n",
    "        dist = max(xmax-xmin, ymax-ymin)\n",
    "        vnrm = vmid + (dist/10)*(th.T[0]/scale)\n",
    "        vpts = np.vstack([vmid, vnrm])\n",
    "        ax.plot(vpts[:,0], vpts[:,1], 'k-', lw=2)\n",
    "        # Try to keep limits from moving around\n",
    "        ax.set_xlim((xmin, xmax))\n",
    "        ax.set_ylim((ymin, ymax))\n",
    "    else:\n",
    "        print('Separator not in plot range')\n",
    "\n",
    "def plot_decision(data, cl, diff=False):\n",
    "    layers = archs(cl)[0]\n",
    "    X, y, model = run_keras_2d(data, layers, 10, trials=1, verbose=False, display=False)\n",
    "    ax = plot_heat(X,y,model)\n",
    "    W = layers[0].get_weights()[0]\n",
    "    W0 = layers[0].get_weights()[1].reshape((cl,1))\n",
    "    if diff:\n",
    "        for i,j in list(itertools.combinations(range(cl),2)):\n",
    "            plot_separator(ax, W[:,i:i+1] - W[:,j:j+1], W0[i:i+1,:] - W0[j:j+1,:])\n",
    "    else:\n",
    "        for i in range(cl):\n",
    "            plot_separator(ax, W[:,i:i+1], W0[i:i+1,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
